import pandas as pd
import jieba
import jieba.analyse
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import numpy as np
import os
from datetime import datetime
from collections import Counter
import re

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("="*80)
print("vivoæ‰‹æœºè¯„è®ºè¯äº‘åˆ†æ".center(80))
print("="*80)

# æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
files = [f for f in os.listdir('.') if f.endswith('å·²æ¸…æ´—.xlsx')]
if not files:
    print("\nâš ï¸  æœªæ‰¾åˆ°å·²æ¸…æ´—çš„æ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶...")
    files = [f for f in os.listdir('.') if f.endswith('_FromTB.xlsx') and not f.startswith('~$')]

if not files:
    print("\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼")
    exit()

latest_file = max(files, key=lambda f: os.path.getmtime(f))
print(f"\nğŸ“ æ•°æ®æ–‡ä»¶: {latest_file}")

# è¯»å–æ•°æ®
df = pd.read_excel(latest_file, sheet_name='å•†å“è¯„è®º')
total_reviews = len(df)
print(f"ğŸ“Š è¯„è®ºæ€»æ•°: {total_reviews:,} æ¡")

# ============================================================================
# ä¸€ã€æ•°æ®é¢„å¤„ç†
# ============================================================================
print(f"\n{'='*80}")
print("ã€ä¸€ã€æ•°æ®é¢„å¤„ç†ä¸åˆ†è¯ã€‘".center(80))
print(f"{'='*80}")

# åˆå¹¶æ‰€æœ‰è¯„è®ºæ–‡æœ¬
all_comments = df['è¯„è®ºå†…å®¹'].dropna().astype(str).tolist()
text_combined = ' '.join(all_comments)

print(f"\nâ–¶ æ–‡æœ¬ç»Ÿè®¡:")
print(f"  æ€»å­—ç¬¦æ•°: {len(text_combined):,}")
print(f"  å¹³å‡è¯„è®ºé•¿åº¦: {len(text_combined)/len(all_comments):.1f} å­—")

# åŠ è½½åœç”¨è¯åˆ—è¡¨
print(f"\nâ–¶ åŠ è½½åœç”¨è¯è¡¨...")
stopwords = set()
stopwords_file = 'stopwords.txt'

if os.path.exists(stopwords_file):
    with open(stopwords_file, 'r', encoding='utf-8') as f:
        for line in f:
            word = line.strip()
            if word:
                stopwords.add(word)
    print(f"  å·²åŠ è½½ {len(stopwords)} ä¸ªåœç”¨è¯")
else:
    print(f"  âš ï¸ æœªæ‰¾åˆ° {stopwords_file}ï¼Œä½¿ç”¨é»˜è®¤åœç”¨è¯")
    # ä½¿ç”¨åŸºæœ¬åœç”¨è¯
    stopwords = set(['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½'])

# æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯ï¼ˆé’ˆå¯¹æ‰‹æœºè¯„è®ºï¼‰
custom_stopwords = set([
    'vivo', 'VIVO', 'Vivo', 'v', 'V', 'x', 'X',
    'æ‰‹æœº', 'éå¸¸', 'æŒº', 'æ¯”è¾ƒ', 'æ„Ÿè§‰', 'è§‰å¾—', 'çœŸçš„', 'ç¡®å®', 'è¿˜æ˜¯',
    'ä¸œè¥¿', 'ä¸œä¸œ', 'å®è´', 'æ”¶åˆ°', 'è¯„ä»·',
    'è›®', 'ä¸€ä¸‹', 'ä¸€ç‚¹', 'æœ‰ç‚¹', 'è¿™ç§', 'é‚£ç§',
    'å“ˆå“ˆ', 'å—¯', 'å—¯å—¯', 'å“ˆ', 'å‘µå‘µ', 'å•¦', 'å“Ÿ', 'å‘€', 'å“‡', 'å˜¿',
])
stopwords.update(custom_stopwords)
print(f"  æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯ {len(custom_stopwords)} ä¸ª")
print(f"  åœç”¨è¯æ€»æ•°: {len(stopwords)}")

# è‡ªå®šä¹‰è¯å…¸ï¼ˆæ·»åŠ ä¸“ä¸šè¯æ±‡ï¼‰
custom_words = [
    'æ€§ä»·æ¯”', 'ç»­èˆª', 'æ‹ç…§', 'åƒç´ ', 'ç”µæ± ', 'å……ç”µ', 'å¿«å……', 'è¿è¡Œ', 'æµç•…',
    'å¤–è§‚', 'é¢œå€¼', 'å±å¹•', 'éŸ³è´¨', 'éŸ³æ•ˆ', 'æ•£çƒ­', 'å‘çƒ­', 'ä¿¡å·', 'æŒ‡çº¹',
    'äººè„¸è¯†åˆ«', 'è§£é”', 'ç³»ç»Ÿ', 'å¤„ç†å™¨', 'å†…å­˜', 'å­˜å‚¨', 'å¡é¡¿', 'æ­»æœº',
    'ç‰©æµ', 'å¿«é€’', 'åŒ…è£…', 'å®¢æœ', 'å”®å', 'æ´»åŠ¨', 'ä¼˜æƒ ', 'åˆ’ç®—', 'è¶…å€¼'
]

for word in custom_words:
    jieba.add_word(word)

print(f"\nâ–¶ åˆ†è¯å¤„ç†ä¸­...")
# åˆ†è¯
words = jieba.lcut(text_combined)

# è¿‡æ»¤
filtered_words = []
for word in words:
    # è¿‡æ»¤æ¡ä»¶
    if (len(word) >= 2 and  # è‡³å°‘2ä¸ªå­—ç¬¦
        word not in stopwords and  # ä¸åœ¨åœç”¨è¯ä¸­
        not word.isdigit() and  # ä¸æ˜¯çº¯æ•°å­—
        not re.match(r'^[a-zA-Z]+$', word) and  # ä¸æ˜¯çº¯è‹±æ–‡
        not re.match(r'^[\W_]+$', word)):  # ä¸æ˜¯çº¯ç¬¦å·
        filtered_words.append(word)

print(f"  åŸå§‹è¯æ•°: {len(words):,}")
print(f"  è¿‡æ»¤åè¯æ•°: {len(filtered_words):,}")
print(f"  å»é™¤ç‡: {(1 - len(filtered_words)/len(words))*100:.1f}%")

# ============================================================================
# äºŒã€è¯é¢‘ç»Ÿè®¡
# ============================================================================
print(f"\n{'='*80}")
print("ã€äºŒã€è¯é¢‘ç»Ÿè®¡ã€‘".center(80))
print(f"{'='*80}")

# ç»Ÿè®¡è¯é¢‘
word_counts = Counter(filtered_words)
top_words = word_counts.most_common(50)

print(f"\nâ–¶ TOP 30 é«˜é¢‘è¯æ±‡:")
print("-" * 80)
print(f"{'æ’å':<6} {'è¯æ±‡':<15} {'é¢‘æ¬¡':<10} {'å æ¯”':<10} {'é¢‘æ¬¡å›¾':<20}")
print("-" * 80)

total_filtered_words = len(filtered_words)
for i, (word, count) in enumerate(top_words[:30], 1):
    percent = count / total_filtered_words * 100
    bar = "â–ˆ" * int(count / top_words[0][1] * 20)
    print(f"{i:<6} {word:<15} {count:<10} {percent:>5.2f}%     {bar}")

# ============================================================================
# ä¸‰ã€ç”Ÿæˆè¯äº‘å›¾
# ============================================================================
print(f"\n{'='*80}")
print("ã€ä¸‰ã€ç”Ÿæˆè¯äº‘å›¾ã€‘".center(80))
print(f"{'='*80}")

print(f"\nâ–¶ æ­£åœ¨ç”Ÿæˆè¯äº‘å›¾...")

# åˆ›å»ºè¯é¢‘å­—å…¸
word_freq = dict(word_counts)

# ç”Ÿæˆè¯äº‘ï¼ˆä½¿ç”¨é»˜è®¤å­—ä½“ï¼‰
wordcloud = WordCloud(
    width=1600,
    height=800,
    background_color='white',
    font_path='C:/Windows/Fonts/simhei.ttf',  # é»‘ä½“
    max_words=200,
    relative_scaling=0.5,
    colormap='viridis',
    min_font_size=10,
    random_state=42
).generate_from_frequencies(word_freq)

# ç»˜åˆ¶è¯äº‘å›¾
fig, axes = plt.subplots(1, 2, figsize=(16, 8))

# å·¦å›¾ï¼šè¯äº‘
ax1 = axes[0]
ax1.imshow(wordcloud, interpolation='bilinear')
ax1.axis('off')
ax1.set_title('è¯„è®ºè¯äº‘å›¾', fontsize=18, fontweight='bold', pad=20)

# å³å›¾ï¼šTOP20è¯é¢‘æŸ±çŠ¶å›¾
ax2 = axes[1]
top20_words = [w[0] for w in top_words[:20]]
top20_counts = [w[1] for w in top_words[:20]]
colors = plt.cm.viridis(np.linspace(0.3, 0.9, 20))

bars = ax2.barh(range(20), top20_counts, color=colors)
ax2.set_yticks(range(20))
ax2.set_yticklabels(top20_words, fontsize=11)
ax2.set_xlabel('å‡ºç°æ¬¡æ•°', fontsize=12)
ax2.set_title('TOP20 é«˜é¢‘è¯æ±‡', fontsize=16, fontweight='bold', pad=15)
ax2.invert_yaxis()

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, (bar, count) in enumerate(zip(bars, top20_counts)):
    ax2.text(count, i, f' {count}', va='center', fontsize=10)

plt.tight_layout()
wordcloud_file = f'è¯äº‘å›¾åˆ†æ_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
plt.savefig(wordcloud_file, dpi=300, bbox_inches='tight')
print(f"âœ… è¯äº‘å›¾å·²ä¿å­˜: {wordcloud_file}")

# ============================================================================
# å››ã€å…³é”®è¯åˆ†ç±»åˆ†æ
# ============================================================================
print(f"\n{'='*80}")
print("ã€å››ã€å…³é”®è¯åˆ†ç±»åˆ†æã€‘".center(80))
print(f"{'='*80}")

# å®šä¹‰å…³é”®è¯åˆ†ç±»
keyword_categories = {
    'æ€§èƒ½ä½“éªŒ': ['æµç•…', 'è¿è¡Œ', 'å¡é¡¿', 'é€Ÿåº¦', 'å¿«', 'æ…¢', 'å¤„ç†å™¨', 'èŠ¯ç‰‡', 'æ€§èƒ½', 'é…ç½®'],
    'å¤–è§‚è®¾è®¡': ['å¤–è§‚', 'é¢œå€¼', 'å¥½çœ‹', 'æ¼‚äº®', 'ç¾', 'é¢œè‰²', 'æ‰‹æ„Ÿ', 'è´¨æ„Ÿ', 'è½»è–„', 'å¤§æ°”'],
    'æ‹ç…§åŠŸèƒ½': ['æ‹ç…§', 'ç›¸æœº', 'æ‘„åƒ', 'ç…§ç‰‡', 'åƒç´ ', 'æ¸…æ™°', 'å¤œæ™¯', 'ç¾é¢œ', 'é•œå¤´'],
    'ç»­èˆªå……ç”µ': ['ç»­èˆª', 'ç”µæ± ', 'å……ç”µ', 'å¿«å……', 'è€ç”¨', 'ç”µé‡', 'æ‰ç”µ', 'è€—ç”µ'],
    'å±å¹•æ˜¾ç¤º': ['å±å¹•', 'æ˜¾ç¤º', 'ç”»è´¨', 'è‰²å½©', 'äº®åº¦', 'æŠ¤çœ¼', 'åˆ†è¾¨ç‡'],
    'ç³»ç»Ÿè½¯ä»¶': ['ç³»ç»Ÿ', 'è½¯ä»¶', 'æ›´æ–°', 'åº”ç”¨', 'ç•Œé¢', 'æ“ä½œ', 'åŠŸèƒ½'],
    'æœåŠ¡ç‰©æµ': ['ç‰©æµ', 'å¿«é€’', 'åŒ…è£…', 'å®¢æœ', 'å”®å', 'æœåŠ¡', 'å‘è´§', 'é…é€'],
    'æ€§ä»·æ¯”': ['æ€§ä»·æ¯”', 'åˆ’ç®—', 'å€¼å¾—', 'è¶…å€¼', 'å®æƒ ', 'ä¾¿å®œ', 'ä»·æ ¼', 'ä¼˜æƒ '],
}

print(f"\nâ–¶ å„ç»´åº¦å…³é”®è¯ç»Ÿè®¡:")
print("-" * 80)

category_stats = {}
for category, keywords in keyword_categories.items():
    count = sum(word_counts.get(kw, 0) for kw in keywords)
    category_stats[category] = count
    
    # æ˜¾ç¤ºè¯¥ç±»åˆ«çš„é«˜é¢‘è¯
    category_words = [(kw, word_counts.get(kw, 0)) for kw in keywords if word_counts.get(kw, 0) > 0]
    category_words.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\nã€{category}ã€‘ æ€»è®¡: {count} æ¬¡")
    if category_words:
        top_5 = category_words[:5]
        top_words_str = 'ã€'.join([f"{w}({c}æ¬¡)" for w, c in top_5])
        print(f"  é«˜é¢‘è¯: {top_words_str}")

# ç»˜åˆ¶åˆ†ç±»ç»Ÿè®¡å›¾
plt.figure(figsize=(12, 6))
categories = list(category_stats.keys())
counts = list(category_stats.values())
colors_cat = plt.cm.Set3(np.linspace(0, 1, len(categories)))

bars = plt.bar(categories, counts, color=colors_cat, edgecolor='black', linewidth=1.5)
plt.xlabel('å…³æ³¨ç»´åº¦', fontsize=12, fontweight='bold')
plt.ylabel('æåŠæ¬¡æ•°', fontsize=12, fontweight='bold')
plt.title('ç”¨æˆ·å…³æ³¨ç»´åº¦åˆ†æ', fontsize=16, fontweight='bold', pad=15)
plt.xticks(rotation=45, ha='right')

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, count in zip(bars, counts):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{int(count)}',
             ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.tight_layout()
category_file = f'å…³é”®è¯åˆ†ç±»ç»Ÿè®¡_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
plt.savefig(category_file, dpi=300, bbox_inches='tight')
print(f"\nğŸ’¾ åˆ†ç±»ç»Ÿè®¡å›¾å·²ä¿å­˜: {category_file}")

# ============================================================================
# äº”ã€ç”Ÿæˆåˆ†ææŠ¥å‘Š
# ============================================================================
print(f"\n{'='*80}")
print("ã€äº”ã€ç”Ÿæˆåˆ†ææŠ¥å‘Šã€‘".center(80))
print(f"{'='*80}")

report_file = f'è¯äº‘åˆ†ææŠ¥å‘Š_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'
with open(report_file, 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("vivoæ‰‹æœºè¯„è®ºè¯äº‘åˆ†ææŠ¥å‘Š\n".center(80))
    f.write("="*80 + "\n\n")
    f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n")
    f.write(f"æ•°æ®æ¥æº: {latest_file}\n")
    f.write(f"æ ·æœ¬æ•°é‡: {total_reviews:,} æ¡è¯„è®º\n\n")
    
    f.write("ä¸€ã€æ–‡æœ¬åˆ†ææ¦‚å†µ\n")
    f.write("-" * 80 + "\n")
    f.write(f"æ€»å­—ç¬¦æ•°: {len(text_combined):,}\n")
    f.write(f"å¹³å‡è¯„è®ºé•¿åº¦: {len(text_combined)/len(all_comments):.1f} å­—\n")
    f.write(f"åˆ†è¯æ€»æ•°: {len(words):,}\n")
    f.write(f"æœ‰æ•ˆè¯æ±‡æ•°: {len(filtered_words):,}\n")
    f.write(f"å»é‡åè¯æ±‡æ•°: {len(word_counts):,}\n\n")
    
    f.write("äºŒã€TOP 50 é«˜é¢‘è¯æ±‡\n")
    f.write("-" * 80 + "\n")
    f.write(f"{'æ’å':<6} {'è¯æ±‡':<15} {'é¢‘æ¬¡':<10} {'å æ¯”':<10}\n")
    f.write("-" * 80 + "\n")
    for i, (word, count) in enumerate(top_words, 1):
        percent = count / total_filtered_words * 100
        f.write(f"{i:<6} {word:<15} {count:<10} {percent:>5.2f}%\n")
    
    f.write("\nä¸‰ã€ç”¨æˆ·å…³æ³¨ç»´åº¦åˆ†æ\n")
    f.write("-" * 80 + "\n")
    sorted_categories = sorted(category_stats.items(), key=lambda x: x[1], reverse=True)
    for i, (category, count) in enumerate(sorted_categories, 1):
        percent = count / sum(category_stats.values()) * 100
        f.write(f"\n{i}. {category} ({count}æ¬¡, å {percent:.1f}%)\n")
        
        # æ˜¾ç¤ºè¯¥ç±»åˆ«çš„é«˜é¢‘è¯
        keywords = keyword_categories[category]
        category_words = [(kw, word_counts.get(kw, 0)) for kw in keywords if word_counts.get(kw, 0) > 0]
        category_words.sort(key=lambda x: x[1], reverse=True)
        if category_words:
            f.write("   å…³é”®è¯: ")
            f.write('ã€'.join([f"{w}({c})" for w, c in category_words[:10]]))
            f.write("\n")
    
    f.write("\nå››ã€å…³é”®å‘ç°\n")
    f.write("-" * 80 + "\n")
    
    # è‡ªåŠ¨ç”Ÿæˆæ´å¯Ÿ
    top_category = sorted_categories[0][0]
    top_category_percent = sorted_categories[0][1] / sum(category_stats.values()) * 100
    f.write(f"1. ç”¨æˆ·æœ€å…³æ³¨ã€{top_category}ã€‘ç»´åº¦ï¼ŒæåŠæ¬¡æ•°å {top_category_percent:.1f}%\n")
    
    # åˆ†æé«˜é¢‘è¯
    if 'æµç•…' in [w[0] for w in top_words[:20]]:
        f.write(f"2. 'æµç•…'ä¸€è¯é«˜é¢‘å‡ºç°ï¼Œè¯´æ˜ç”¨æˆ·å¯¹ç³»ç»Ÿæµç•…åº¦éå¸¸å…³æ³¨\n")
    
    if 'å¤–è§‚' in [w[0] for w in top_words[:20]] or 'é¢œå€¼' in [w[0] for w in top_words[:20]]:
        f.write(f"3. å¤–è§‚é¢œå€¼æ˜¯ç”¨æˆ·é‡ç‚¹è¯„ä»·å¯¹è±¡ï¼Œå»ºè®®åŠ å¼ºå¤–è§‚è®¾è®¡å®£ä¼ \n")
    
    if category_stats.get('ç»­èˆªå……ç”µ', 0) > category_stats.get('æ‹ç…§åŠŸèƒ½', 0):
        f.write(f"4. ç”¨æˆ·å¯¹ç»­èˆªçš„å…³æ³¨åº¦è¶…è¿‡æ‹ç…§åŠŸèƒ½\n")
    
    f.write("\n" + "="*80 + "\n")
    f.write("æŠ¥å‘Šç”Ÿæˆå®Œæ¯•\n")

print(f"\nâœ… åˆ†æå®Œæˆï¼")
print(f"   - è¯äº‘å›¾æ–‡ä»¶: {wordcloud_file}")
print(f"   - åˆ†ç±»ç»Ÿè®¡å›¾: {category_file}")
print(f"   - åˆ†ææŠ¥å‘Š: {report_file}")
print(f"\n{'='*80}")
